{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adf8ec-b58e-4f61-9f44-0f54c1960a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training GMM Network -----\n",
      "[GMM] Epoch 1/200 | Loss: 0.1030\n",
      "[GMM] Epoch 2/200 | Loss: -0.2014\n",
      "[GMM] Epoch 3/200 | Loss: -0.1944\n",
      "[GMM] Epoch 4/200 | Loss: -0.1652\n",
      "[GMM] Epoch 5/200 | Loss: -0.2943\n",
      "[GMM] Epoch 6/200 | Loss: -0.2308\n",
      "[GMM] Epoch 7/200 | Loss: -0.6756\n",
      "[GMM] Epoch 8/200 | Loss: -1.0311\n",
      "[GMM] Epoch 9/200 | Loss: -1.1272\n",
      "[GMM] Epoch 10/200 | Loss: -1.0066\n",
      "[GMM] Epoch 11/200 | Loss: -0.8172\n",
      "[GMM] Epoch 12/200 | Loss: -1.6351\n",
      "[GMM] Epoch 13/200 | Loss: -1.4727\n",
      "[GMM] Epoch 14/200 | Loss: -1.5051\n",
      "[GMM] Epoch 15/200 | Loss: -1.3422\n",
      "[GMM] Epoch 16/200 | Loss: -1.5470\n",
      "[GMM] Epoch 17/200 | Loss: -2.1470\n",
      "[GMM] Epoch 18/200 | Loss: -2.0530\n",
      "[GMM] Epoch 19/200 | Loss: -1.7525\n",
      "[GMM] Epoch 20/200 | Loss: -1.2383\n",
      "[GMM] Epoch 21/200 | Loss: -2.2389\n",
      "[GMM] Epoch 22/200 | Loss: -2.3577\n",
      "[GMM] Epoch 23/200 | Loss: -1.4248\n",
      "[GMM] Epoch 24/200 | Loss: -2.3639\n",
      "[GMM] Epoch 25/200 | Loss: -2.5661\n",
      "[GMM] Epoch 26/200 | Loss: -2.4741\n",
      "[GMM] Epoch 27/200 | Loss: -2.1473\n",
      "[GMM] Epoch 28/200 | Loss: -0.7775\n",
      "[GMM] Epoch 29/200 | Loss: -1.2583\n",
      "[GMM] Epoch 30/200 | Loss: -1.4909\n",
      "[GMM] Epoch 31/200 | Loss: -1.9759\n",
      "[GMM] Epoch 32/200 | Loss: -2.4236\n",
      "[GMM] Epoch 33/200 | Loss: -2.2885\n",
      "[GMM] Epoch 34/200 | Loss: -2.5498\n",
      "[GMM] Epoch 35/200 | Loss: -2.2965\n",
      "[GMM] Epoch 36/200 | Loss: -2.1499\n",
      "[GMM] Epoch 37/200 | Loss: -2.3761\n",
      "[GMM] Epoch 38/200 | Loss: -2.5137\n",
      "[GMM] Epoch 39/200 | Loss: -2.4314\n",
      "[GMM] Epoch 40/200 | Loss: -2.0698\n",
      "[GMM] Epoch 41/200 | Loss: -2.4340\n",
      "[GMM] Epoch 42/200 | Loss: -2.3986\n",
      "[GMM] Epoch 43/200 | Loss: -2.1859\n",
      "[GMM] Epoch 44/200 | Loss: -2.4271\n",
      "[GMM] Epoch 45/200 | Loss: -2.6024\n",
      "[GMM] Epoch 46/200 | Loss: -2.5191\n",
      "[GMM] Epoch 47/200 | Loss: -2.7646\n",
      "[GMM] Epoch 48/200 | Loss: -1.8517\n",
      "[GMM] Epoch 49/200 | Loss: -2.2257\n",
      "[GMM] Epoch 50/200 | Loss: -1.9303\n",
      "[GMM] Epoch 51/200 | Loss: -2.3237\n",
      "[GMM] Epoch 52/200 | Loss: -2.4339\n",
      "[GMM] Epoch 53/200 | Loss: -1.6001\n",
      "[GMM] Epoch 54/200 | Loss: -1.9316\n",
      "[GMM] Epoch 55/200 | Loss: -2.2304\n",
      "[GMM] Epoch 56/200 | Loss: -2.3365\n",
      "[GMM] Epoch 57/200 | Loss: -2.1607\n",
      "[GMM] Epoch 58/200 | Loss: -2.4429\n",
      "[GMM] Epoch 59/200 | Loss: -2.6559\n",
      "[GMM] Epoch 60/200 | Loss: -2.7285\n",
      "[GMM] Epoch 61/200 | Loss: -2.8508\n",
      "[GMM] Epoch 62/200 | Loss: -2.8882\n",
      "[GMM] Epoch 63/200 | Loss: -2.8723\n",
      "[GMM] Epoch 64/200 | Loss: -2.9097\n",
      "[GMM] Epoch 65/200 | Loss: -2.9896\n",
      "[GMM] Epoch 66/200 | Loss: -3.0479\n",
      "[GMM] Epoch 67/200 | Loss: -2.8819\n",
      "[GMM] Epoch 68/200 | Loss: -3.0998\n",
      "[GMM] Epoch 69/200 | Loss: -3.0023\n",
      "[GMM] Epoch 70/200 | Loss: -2.9487\n",
      "[GMM] Epoch 71/200 | Loss: -2.9191\n",
      "[GMM] Epoch 72/200 | Loss: -2.3389\n",
      "[GMM] Epoch 73/200 | Loss: -2.7895\n",
      "[GMM] Epoch 74/200 | Loss: -2.9293\n",
      "[GMM] Epoch 75/200 | Loss: -2.7491\n",
      "[GMM] Epoch 76/200 | Loss: -3.0492\n",
      "[GMM] Epoch 77/200 | Loss: -3.0663\n",
      "[GMM] Epoch 78/200 | Loss: -3.0457\n",
      "[GMM] Epoch 79/200 | Loss: -2.9355\n",
      "[GMM] Epoch 80/200 | Loss: -3.0473\n",
      "[GMM] Epoch 81/200 | Loss: -3.1071\n",
      "[GMM] Epoch 82/200 | Loss: -2.9690\n",
      "[GMM] Epoch 83/200 | Loss: -3.0044\n",
      "[GMM] Epoch 84/200 | Loss: -3.0376\n",
      "[GMM] Epoch 85/200 | Loss: -2.9754\n",
      "[GMM] Epoch 86/200 | Loss: -2.9696\n",
      "[GMM] Epoch 87/200 | Loss: -3.0334\n",
      "[GMM] Epoch 88/200 | Loss: -3.0695\n",
      "[GMM] Epoch 89/200 | Loss: -3.0153\n",
      "[GMM] Epoch 90/200 | Loss: -3.0987\n",
      "[GMM] Epoch 91/200 | Loss: -3.0653\n",
      "[GMM] Epoch 92/200 | Loss: -3.0784\n",
      "[GMM] Epoch 93/200 | Loss: -3.0721\n",
      "[GMM] Epoch 94/200 | Loss: -3.0827\n",
      "[GMM] Epoch 95/200 | Loss: -2.9495\n",
      "[GMM] Epoch 96/200 | Loss: -3.0498\n",
      "[GMM] Epoch 97/200 | Loss: -3.0705\n",
      "[GMM] Epoch 98/200 | Loss: -3.0677\n",
      "[GMM] Epoch 99/200 | Loss: -3.0333\n",
      "[GMM] Epoch 100/200 | Loss: -3.0874\n",
      "[GMM] Epoch 101/200 | Loss: -3.0475\n",
      "[GMM] Epoch 102/200 | Loss: -3.0140\n",
      "[GMM] Epoch 103/200 | Loss: -3.0125\n",
      "[GMM] Epoch 104/200 | Loss: -1.3640\n",
      "[GMM] Epoch 105/200 | Loss: -2.4340\n",
      "[GMM] Epoch 106/200 | Loss: -1.1306\n",
      "[GMM] Epoch 107/200 | Loss: -2.2312\n",
      "[GMM] Epoch 108/200 | Loss: -0.2355\n",
      "[GMM] Epoch 109/200 | Loss: -2.1387\n",
      "[GMM] Epoch 110/200 | Loss: -2.7157\n",
      "[GMM] Epoch 111/200 | Loss: -2.8283\n",
      "[GMM] Epoch 112/200 | Loss: -2.9134\n",
      "[GMM] Epoch 113/200 | Loss: -2.8940\n",
      "[GMM] Epoch 114/200 | Loss: -2.9361\n",
      "[GMM] Epoch 115/200 | Loss: -3.0377\n",
      "[GMM] Epoch 116/200 | Loss: -2.9149\n",
      "[GMM] Epoch 117/200 | Loss: -3.0630\n",
      "[GMM] Epoch 118/200 | Loss: -3.0648\n",
      "[GMM] Epoch 119/200 | Loss: -3.0827\n",
      "[GMM] Epoch 120/200 | Loss: -3.0909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import beta, norm, ks_2samp, wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "def generate_theta_y_n_large(batch_size=256, n_min=50, n_max=100_000, log_uniform=True):\n",
    "    if log_uniform:\n",
    "        log_n = np.random.uniform(np.log(n_min), np.log(n_max), size=(batch_size, 1))\n",
    "        n = np.round(np.exp(log_n)).astype(np.float32)\n",
    "    else:\n",
    "        n = np.random.randint(n_min, n_max + 1, size=(batch_size, 1)).astype(np.float32)\n",
    "    theta = np.random.uniform(0, 1, size=(batch_size, 1)).astype(np.float32)\n",
    "    y = np.random.binomial(n=n.astype(int), p=theta).astype(np.float32)\n",
    "    return y, n, theta\n",
    "\n",
    "def gmm_log_likelihood(theta, logits, mus, log_sigmas):\n",
    "    log_sigmas = torch.clamp(log_sigmas, min=-5, max=3)\n",
    "    sigmas = torch.exp(log_sigmas)\n",
    "    pi = torch.softmax(logits, dim=1)\n",
    "    th = theta.unsqueeze(1).expand(-1, mus.size(1))\n",
    "    logp = -0.5 * ((th - mus) / sigmas) ** 2 - log_sigmas - 0.5 * np.log(2 * np.pi)\n",
    "    wlogp = logp + torch.log(pi + 1e-10)\n",
    "    return -torch.mean(torch.logsumexp(wlogp, dim=1))\n",
    "\n",
    "class GMMInferenceNet(nn.Module):\n",
    "    def __init__(self, num_components=3, hidden_dims=(512, 512, 256, 128)):\n",
    "        super().__init__()\n",
    "        self.K = num_components\n",
    "        layers = []\n",
    "        d = 2\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers.append(nn.Linear(d, 3 * self.K))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out[:, :self.K], out[:, self.K:2*self.K], out[:, 2*self.K:]\n",
    "\n",
    "def train_gmm_network(model, optimizer, num_epochs=200, batch_size=256, n_min=50, n_max=100_000, steps_per_epoch=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for _ in range(steps_per_epoch):\n",
    "            y_np, n_np, theta_np = generate_theta_y_n_large(batch_size, n_min, n_max)\n",
    "            x = torch.cat([\n",
    "                torch.from_numpy(y_np / n_np),\n",
    "                torch.from_numpy(np.log(n_np))\n",
    "            ], dim=1).float()\n",
    "            theta = torch.from_numpy(theta_np).squeeze(1).float()\n",
    "            logits, mus, logs = model(x)\n",
    "            pi = torch.softmax(logits, dim=1)\n",
    "            nll = gmm_log_likelihood(theta, logits, mus, logs)\n",
    "            entropy = -torch.sum(pi * torch.log(pi + 1e-8), dim=1).mean()\n",
    "            loss = nll - 0.01 * entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[GMM] Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / steps_per_epoch:.4f}\")\n",
    "\n",
    "def evaluate_gmm_fixed_n(model, fixed_n, y_values, theta_grid, num_components=3):\n",
    "    plt.figure(figsize=(10, 3.5 * len(y_values)))\n",
    "    for i, y in enumerate(y_values):\n",
    "        alpha, beta_param = y + 1, fixed_n - y + 1\n",
    "        true_pdf = beta.pdf(theta_grid, alpha, beta_param)\n",
    "        x = torch.tensor([[y / fixed_n, np.log(fixed_n)]], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits, mus, logs = model(x)\n",
    "        pi = torch.softmax(logits, dim=1).numpy().flatten()\n",
    "        mus = mus.numpy().flatten()\n",
    "        sig = np.exp(logs.numpy()).flatten()\n",
    "        gmm_pdf = sum(pi[k] * norm.pdf(theta_grid, mus[k], sig[k]) for k in range(num_components))\n",
    "        ts = beta.rvs(alpha, beta_param, size=2000)\n",
    "        ss = np.clip(np.hstack([\n",
    "            np.random.normal(mus[k], sig[k], int(round(pi[k] * 2000)))\n",
    "            for k in range(num_components)\n",
    "        ]), 0, 1)\n",
    "        ks, p = ks_2samp(ts, ss)\n",
    "        wd = wasserstein_distance(ts, ss)\n",
    "        ax = plt.subplot(len(y_values), 1, i + 1)\n",
    "        ax.plot(theta_grid, true_pdf, 'b--')\n",
    "        ax.plot(theta_grid, gmm_pdf, 'r-')\n",
    "        ax.set_title(f\"GMM Fixed n={fixed_n}, y={y} | KS={ks:.3f}, p={p:.3f}, W={wd:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/gmm_fixedN_{fixed_n}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_gmm_unseen_n(model, test_cases, theta_grid, num_components=3):\n",
    "    plt.figure(figsize=(10, 3.5 * len(test_cases)))\n",
    "    for i, (n, y) in enumerate(test_cases):\n",
    "        alpha, beta_param = y + 1, n - y + 1\n",
    "        true_pdf = beta.pdf(theta_grid, alpha, beta_param)\n",
    "        x = torch.tensor([[y / n, np.log(n)]], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits, mus, logs = model(x)\n",
    "        pi = torch.softmax(logits, dim=1).numpy().flatten()\n",
    "        mus = mus.numpy().flatten()\n",
    "        sig = np.exp(logs.numpy()).flatten()\n",
    "        gmm_pdf = sum(pi[k] * norm.pdf(theta_grid, mus[k], sig[k]) for k in range(num_components))\n",
    "        ts = beta.rvs(alpha, beta_param, size=2000)\n",
    "        ss = np.clip(np.hstack([\n",
    "            np.random.normal(mus[k], sig[k], int(round(pi[k] * 2000)))\n",
    "            for k in range(num_components)\n",
    "        ]), 0, 1)\n",
    "        ks, p = ks_2samp(ts, ss)\n",
    "        wd = wasserstein_distance(ts, ss)\n",
    "        ax = plt.subplot(len(test_cases), 1, i + 1)\n",
    "        ax.plot(theta_grid, true_pdf, 'b--')\n",
    "        ax.plot(theta_grid, gmm_pdf, 'r-')\n",
    "        ax.set_title(f\"GMM Unseen n={n}, y={y} | KS={ks:.3f}, p={p:.3f}, W={wd:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/gmm_unseenN.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "K = 10  \n",
    "hidden_dims = (512, 512, 256, 128)\n",
    "gmm_model = GMMInferenceNet(num_components=K, hidden_dims=hidden_dims)\n",
    "gmm_optimizer = optim.Adam(gmm_model.parameters(), lr=1e-3)\n",
    "print(\"----- Training GMM Network -----\")\n",
    "train_gmm_network(gmm_model, gmm_optimizer)\n",
    "\n",
    "theta_grid = np.linspace(0, 1, 500)\n",
    "y_values_fixed = [1, 100, 2000, 8000]\n",
    "test_cases_unseen = [(20000, 1), (50000, 25000), (200000, 150000)]\n",
    "\n",
    "print(\"----- Evaluating GMM Fixed n=10000 -----\")\n",
    "evaluate_gmm_fixed_n(gmm_model, fixed_n=10000, y_values=y_values_fixed, theta_grid=theta_grid, num_components=K)\n",
    "print(\"----- Evaluating GMM Unseen n's -----\")\n",
    "evaluate_gmm_unseen_n(gmm_model, test_cases_unseen, theta_grid, num_components=K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244171ca-b477-484c-9855-440fa8bcaf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
